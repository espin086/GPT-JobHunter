{"date": "2024-02-22T00:00:00.000Z", "title": "Data Engineer", "company": "CubeSmart", "job_apply_link": "https://technical.ly/job/data-analytics/philly/cubesmart-data-engineer-195319/", "company_url": "http://www.cubesmart.com", "company_type": "Real Estate", "job_type": "FULLTIME", "job_is_remote": "Not Remote", "job_offer_expiration_date": null, "salary_low": null, "salary_high": null, "salary_currency": null, "salary_period": null, "job_benefits": null, "city": "Philadelphia", "state": "PA", "country": "US", "apply_options": "https://technical.ly/job/data-analytics/philly/cubesmart-data-engineer-195319/\nhttps://pennsylvania.tarta.ai/j/daLi5I0BcRajQcTjuad70224-data-engineer-at-cubesmart", "required_skills": null, "required_experience": "no_experience_required: False, \nrequired_experience_in_months: None, \nexperience_mentioned: True, \nexperience_preferred: True", "required_education": "postgraduate_degree: False, \nprofessional_certification: False, \nhigh_school: False, \nassociates_degree: False, \nbachelors_degree: False, \ndegree_mentioned: True, \ndegree_preferred: True, \nprofessional_certification_mentioned: False", "description": "This is a hybrid role based out of Malvern, PA.\nCubeSmart is currently seeking a Data Engineer to join the Information Technology team at our corporate office in Malvern, PA. The Data Engineer will be responsible for the implementation, development, and optimization of database, data objects, and database scripting with the core database platforms utilized by the core CubeSmart business applications.\nWho we are:\nAt CubeSmart, we\u2019re intentional about culture. You can experience it everywhere from our mission statement of \u201cgenuine care\u201d to our \u201cIt\u2019s What\u2019s Inside That Counts\u201d tagline to calling each other \u201cteammates\u201d rather than employees. This spirit fosters a fun and collaborative environment that has resulted in our rapid growth and being recognized amongst the top in our industry.\nCubeSmart\u2019s award-winning team is made up of people who genuinely care. Teammates care about our customers and the life events and/or business needs they are facing. Teammates are passionate, responsible and understanding. The CubeSmart team is made up of people who have a can-do attitude, are committed to their own success and the success of the company, and lead by example.\nIf this sounds like a team and culture that matches your personal values and motivations, we want to hear from you.\nThis role will also be responsible for the development and maintenance of ETL processes that manage the consumption of data by CubeSmart\u2019s core systems. The role will serve as a technical expert to analyze the needs of CubeSmart\u2019s applications and business data requirements and produce an optimal solution according to their needs and specifications, and in alignment with CubeSmart\u2019s data design standards. This individual will be part of the team building a data and analytics platform in the Microsoft Azure cloud to support the CubeSmart business. This will include designing and building the data ingestion and ETL pipelines to create the cloud Lakehouse environment. In addition, this person will be engaged to develop features in the legacy SQL Server Data Warehouse platform.\nResponsibilities\n\nCollaborate with program manager(s), application development, and data consumers to develop data ingestion and ETL pipelines required to enable CubeSmart data consumers to onboard processes to the Data Lakehouse platform\nCreate database designs, stored procedures, views and other associated database objects required to implement processes in support of application development efforts\nDesign and development of data pipelines and ETL processes required to support system integration and application development needs in the legacy SQL data warehouse environment\nIdentification and implementation of database performance tuning required to validate that all systems are performing to an optimal level\nCollaborate with 3rd party vendors to understand externally sourced data and incorporate into CubeSmart\u2019s data environment, in adherence with CubeSmart\u2019s data design standards\n\nBachelor\u2019s degree in Computer Science, Information Technology or a related discipline\nStrong skills in SQL, Python, and PySpark\nExperience with Microsoft Azure data services (Azure Data Factory, Azure Databricks, Azure SQL, and Databricks ML Flow) or equivalent is preferred\nFull understanding of the Software Development Life Cycle (SDLC) and experience with Agile software development processes\nSolid understanding of cloud DevOps capabilities, preferably Azure DevOps\nExperience with MongoDB (or similar non-relational database platform) or PostgreSQL is a plus\nExperience supporting databases and data models for ERP or financial systems in a plus\n\nKnowledge, Skills, Abilities and Personal Characteristics\n\nService orientation towards business-focused 24\u00d77 support and service mentality\nAbility to work within a team environment, showing an openness to collaborate with technical and business peer, with the ability to step up and take on new challenges in response to changing business conditions\nAbility to learn quickly, adapt work processes to adhere with best practices\nAbility to clearly convey technical information to both technical and non-technical audiences\nCollaborative individual who creates open channels of communications and encourages technical dialogue across the department.\nWell-developed analytical and problem-solving abilities\nAbility to work on multiple tasks and projects at once, with the ability to properly prioritize one\u2019s own work and the work of others", "highlights": "\nQualifications:\n Bachelor\u2019s degree in Computer Science, Information Technology or a related discipline, Strong skills in SQL, Python, and PySpark, Full understanding of the Software Development Life Cycle (SDLC) and experience with Agile software development processes, Solid understanding of cloud DevOps capabilities, preferably Azure DevOps, Knowledge, Skills, Abilities and Personal Characteristics, Service orientation towards business-focused 24\u00d77 support and service mentality, Ability to work within a team environment, showing an openness to collaborate with technical and business peer, with the ability to step up and take on new challenges in response to changing business conditions, Ability to learn quickly, adapt work processes to adhere with best practices, Ability to clearly convey technical information to both technical and non-technical audiences, Collaborative individual who creates open channels of communications and encourages technical dialogue across the department, Well-developed analytical and problem-solving abilities, Ability to work on multiple tasks and projects at once, with the ability to properly prioritize one\u2019s own work and the work of others, \nResponsibilities:\n This role will also be responsible for the development and maintenance of ETL processes that manage the consumption of data by CubeSmart\u2019s core systems, The role will serve as a technical expert to analyze the needs of CubeSmart\u2019s applications and business data requirements and produce an optimal solution according to their needs and specifications, and in alignment with CubeSmart\u2019s data design standards, This individual will be part of the team building a data and analytics platform in the Microsoft Azure cloud to support the CubeSmart business, This will include designing and building the data ingestion and ETL pipelines to create the cloud Lakehouse environment, In addition, this person will be engaged to develop features in the legacy SQL Server Data Warehouse platform, Collaborate with program manager(s), application development, and data consumers to develop data ingestion and ETL pipelines required to enable CubeSmart data consumers to onboard processes to the Data Lakehouse platform, Create database designs, stored procedures, views and other associated database objects required to implement processes in support of application development efforts, Identification and implementation of database performance tuning required to validate that all systems are performing to an optimal level", "resume_similarity": null}