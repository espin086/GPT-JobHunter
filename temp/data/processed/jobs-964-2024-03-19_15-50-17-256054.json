{"date": "2024-03-01T00:00:00.000Z", "title": "Data Engineer", "company": "MongoDB", "job_apply_link": "https://www.mongodb.com/careers/jobs/5775138", "company_url": "http://www.mongodb.com", "company_type": "Information", "job_type": "FULLTIME", "job_is_remote": "Not Remote", "job_offer_expiration_date": null, "salary_low": null, "salary_high": null, "salary_currency": null, "salary_period": null, "job_benefits": null, "city": "New York", "state": "NY", "country": "US", "apply_options": "https://www.mongodb.com/careers/jobs/5775138\nhttps://wellfound.com/jobs/2948693-data-engineer\nhttps://echojobs.io/job/mongodb-data-engineer-sqpkw\nhttps://www.talent.com/view?id=d7ace23a950d\nhttps://us.bebee.com/job/20240309-8a83657ca54d15d306224595ba93618a\nhttps://lensa.com/job/mongodb/new-york-ny/data-engineer/b1784308f90dfe5d9666063b3e1b3695\nhttps://hiring-jobs.com/job/data-engineer-17/\nhttps://skilledworkerjobs.com/job/data-engineer-99/", "required_skills": null, "required_experience": "no_experience_required: False, \nrequired_experience_in_months: 24, \nexperience_mentioned: True, \nexperience_preferred: True", "required_education": "postgraduate_degree: False, \nprofessional_certification: False, \nhigh_school: False, \nassociates_degree: False, \nbachelors_degree: False, \ndegree_mentioned: False, \ndegree_preferred: False, \nprofessional_certification_mentioned: False", "description": "The worldwide data management software market is massive (According to IDC, the worldwide database software market, which it refers to as the database management systems software market, was forecasted to be approximately $82 billion in 2023 growing to approximately $137 billion in 2027. This represents a 14% compound annual growth rate). At MongoDB we are transforming industries and empowering developers to build amazing apps that people use every day. We are the leading developer data platform and the first database provider to IPO in over 20 years. Join our team and be at the forefront of innovation and creativity.\n\nThe Data Pipelines Engineering team is responsible for building ETL pipelines that populate the Internal Data Platform, which drives analytics that help the company run more efficiently. Our team builds highly performant and scalable processes that extract massive datasets and makes those datasets available for querying in an optimal way. We are also building a Generative AI framework that will help teams within the company tap into the data that we store in their Retrieval-Augmented Generation (RAG)-based applications.\n\nWe are looking to speak to candidates who are based in New York City for our hybrid working model.\n\nWhat you\u2019ll do:\n\u2022 Build ETL pipelines using technologies such as Python and Spark\n\u2022 Implement new ETL pipelines on top of a variety of architectures (e.g. file-based, streaming)\n\u2022 Optimally store large datasets using a variety of file formats (e.g. Parquet, JSON) and table types (e.g. Iceberg, Hive)\n\u2022 Work with Data Analysts and Data Scientists to understand and make available the data that is important for their analysis\n\u2022 Work with our Data Platform, Architecture, and Governance sibling teams to make data scalable, consumable, and discoverable\n\u2022 Leverage Cloud-based technologies (mostly AWS, some GCP) to build and deploy data pipelines\n\nWe\u2019re looking for someone with:\n\u2022 2+ years of building ETL pipelines for a Data Lake/Warehouse\n\u2022 2+ years Python experience\n\u2022 2+ years Spark experience\n\nHive, Iceberg, Glue, or other technologies that expose big data as tables\n\u2022 Familiarity with different big data file types such as Parquet, Avro, and JSON\n\u2022 Background in building data platforms in the Cloud (e.g. AWS, GCP, Azure)\n\u2022 Experience building RAG-based tools is a plus\n\nSuccess Measures:\n\u2022 In 3 months, you'll have collaborated with stakeholders in Data Analytics and Data Science to build your first ETL pipeline\n\u2022 In 6 months, you'll have owned the delivery of a large project from start (scoping, design) to finish (delivery)\n\u2022 In 12 months, you'll have designed new features, led development work, and become a go-to expert on parts of the system\n\nTo drive the personal growth and business impact of our employees, we\u2019re committed to developing a supportive and enriching culture for everyone. From employee affinity groups, to fertility assistance and a generous parental leave policy, we value our employees\u2019 wellbeing and want to support them along every step of their professional and personal journeys. Learn more about what it\u2019s like to work at MongoDB, and help us make an impact on the world!\n\nMongoDB is committed to providing any necessary accommodations for individuals with disabilities within our application and interview process. To request an accommodation due to a disability, please inform your recruiter.\n\nMongoDB, Inc. provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type and makes all hiring decisions without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.", "highlights": "\nQualifications:\n 2+ years of building ETL pipelines for a Data Lake/Warehouse, 2+ years Python experience, 2+ years Spark experience, Hive, Iceberg, Glue, or other technologies that expose big data as tables, Familiarity with different big data file types such as Parquet, Avro, and JSON, Background in building data platforms in the Cloud (e.g, AWS, GCP, Azure), In 3 months, you'll have collaborated with stakeholders in Data Analytics and Data Science to build your first ETL pipeline, \nResponsibilities:\n Build ETL pipelines using technologies such as Python and Spark, Implement new ETL pipelines on top of a variety of architectures (e.g. file-based, streaming), Optimally store large datasets using a variety of file formats (e.g. Parquet, JSON) and table types (e.g, Work with Data Analysts and Data Scientists to understand and make available the data that is important for their analysis, Work with our Data Platform, Architecture, and Governance sibling teams to make data scalable, consumable, and discoverable, In 6 months, you'll have owned the delivery of a large project from start (scoping, design) to finish (delivery)", "resume_similarity": null}