{"date": "2024-02-27T00:01:45.000Z", "title": "Apache Flink Expert", "company": "Promoted.ai", "job_apply_link": "https://www.ycombinator.com/companies/promoted/jobs/1dLskWr-apache-flink-expert", "company_url": "http://www.promoted.ai", "company_type": null, "job_type": "FULLTIME", "job_is_remote": "Remote", "job_offer_expiration_date": null, "salary_low": 200000, "salary_high": 260000, "salary_currency": "USD", "salary_period": "YEAR", "job_benefits": null, "city": "San Francisco", "state": "CA", "country": "US", "apply_options": "https://www.ycombinator.com/companies/promoted/jobs/1dLskWr-apache-flink-expert\nhttps://www.workatastartup.com/jobs/64819", "required_skills": null, "required_experience": "no_experience_required: False, \nrequired_experience_in_months: None, \nexperience_mentioned: True, \nexperience_preferred: False", "required_education": "postgraduate_degree: False, \nprofessional_certification: False, \nhigh_school: False, \nassociates_degree: False, \nbachelors_degree: False, \ndegree_mentioned: False, \ndegree_preferred: False, \nprofessional_certification_mentioned: False", "description": "Data Stream Processing Software Engineer\n\nShip innovations in streaming data processing at publicly traded company data scales but at startup scale shipping velocities!\n\nResponsibilities\n\u2022 Leverage Apache Flink to develop robust and efficient stream processing jobs and tackle challenges related to data skew, out-of-order events, and other intricacies of streaming data.\n\u2022 Architect and implement near real-time ingestion pipelines for data lakes.\n\u2022 Maintain and optimize the data infrastructure for streaming systems.\n\u2022 Collaborate closely with cross-functional team members to ensure seamless data processing.\n\nRequirements\n\u2022 Enthusiasm for working in a startup environment with a small, dedicated team.\n\u2022 Proficiency in at least one programming language, such as Java, Scala, Kotlin, or Python.\n\u2022 Hands-on experience with big data systems such as Apache Spark, Apache Flink, and Apache Kafka.\n\u2022 Knowledge of common data storage formats such as Avro, Proto, and Parquet.\n\u2022 Experience with at least one cloud platform, such as AWS or GCP.\n\u2022 Familiarity with Kubernetes and experience with tools such as Helm, Prometheus, and Grafana.\n\nPreferred Qualifications\n\u2022 Familiarity with data analytics tools such as Apache Hive, ClickHouse, Presto, Trino, or AWS Redshift.\n\u2022 Experience data lake formats such as Apache Iceberg and Apache Hudi.\n\u2022 Experience with DevOps practices, IoC, and Terraform.\n\u2022 Background in Machine Learning or Recommendation Systems.", "highlights": "\nQualifications:\n Enthusiasm for working in a startup environment with a small, dedicated team, Proficiency in at least one programming language, such as Java, Scala, Kotlin, or Python, Hands-on experience with big data systems such as Apache Spark, Apache Flink, and Apache Kafka, Knowledge of common data storage formats such as Avro, Proto, and Parquet, Experience with at least one cloud platform, such as AWS or GCP, Familiarity with Kubernetes and experience with tools such as Helm, Prometheus, and Grafana, \nResponsibilities:\n Leverage Apache Flink to develop robust and efficient stream processing jobs and tackle challenges related to data skew, out-of-order events, and other intricacies of streaming data, Architect and implement near real-time ingestion pipelines for data lakes, Maintain and optimize the data infrastructure for streaming systems, Collaborate closely with cross-functional team members to ensure seamless data processing", "resume_similarity": null}