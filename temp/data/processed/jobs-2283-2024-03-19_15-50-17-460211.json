{"date": "2023-03-15T00:08:03.000Z", "title": "Google Cloud Architect", "company": "Hire Force Global", "job_apply_link": "https://hireforceglobal.com/jobs/google-cloud-architect/", "company_url": null, "company_type": null, "job_type": "FULLTIME", "job_is_remote": "Not Remote", "job_offer_expiration_date": null, "salary_low": null, "salary_high": null, "salary_currency": null, "salary_period": null, "job_benefits": null, "city": null, "state": null, "country": "US", "apply_options": "https://hireforceglobal.com/jobs/google-cloud-architect/", "required_skills": null, "required_experience": "no_experience_required: False, \nrequired_experience_in_months: 48, \nexperience_mentioned: True, \nexperience_preferred: False", "required_education": "postgraduate_degree: False, \nprofessional_certification: False, \nhigh_school: False, \nassociates_degree: False, \nbachelors_degree: False, \ndegree_mentioned: False, \ndegree_preferred: False, \nprofessional_certification_mentioned: True", "description": "Google Cloud Architect\n\nJOB DESCRIPTION\n\nSkills:\n\n1. Hands-on experience in GCP Suite (4+ years minimum) such as Tera Data, Big Query, PubSub, Data Fusion, Google Cloud Storage, Dataproc, Composer, and Looker.\n\n2. Solid understanding of Google Cloud architecture\n\n3. Strong work experience in SQL/Data warehousing\n\n4. 4+ years of Working experience on tools like Spark, HBase, Sqoop, Impala, Kafka, Flume, Oozie, MapReduce, etc.\n\n5. GCP Architect certification will be an advantage.\n\n6. Google cloud Platform GCP Compute, Storage, Data, Network, Security,\n\n7. Devops and Big Data. AI ML services on Google Cloud Bigquery,\n\n8. Cloud Data Flow, Cloud Pub Sub, Cloud BigTable, ML APIs, AutoML\n\n9. Cloud ML Serverless Architecture and Security Cloud Functions, Cloud IAM\n\nJOB Description:\n\n\u2022 Design and build data engineering solutions using Google Cloud Platform (GCP) services: Tera data, BigQuery, DataFlow, Pub/Sub, BigTable, Data Fusion, and DataProc.\n\n\u2022 Monitoring and improving the quality of the components throughout the development life cycle\n\n\u2022 Designing and Developing data modules that convert underlying raw data to more readily usable formats for reporting\n\n\u2022 Performing data deep dives to pinpoint Problems\n\n\u2022 Identify the application bottlenecks and opportunities to optimize performance and optimize the performance\n\n\u2022 Work on extracting, Loading, Transforming, cleaning, and validating data using cloud ETL/ELT tools\n\n\u2022 Work with the team to conduct training workshops to identify data sources, flows, and requirements\n\n\u2022 Preparing required project documentation and tracking and reporting regularly on the status of projects to all project stakeholders\n\n\u2022 Working with Business Analysts and other business teams to get the required data to build the semantic views.\n\n\u2022 Periodically update senior management with the status of the project with excellent written and verbal communication skills\n\n\u2022 Troubleshoot production issues and coordinate with the support team for code deployment.", "highlights": "\nQualifications:\n Hands-on experience in GCP Suite (4+ years minimum) such as Tera Data, Big Query, PubSub, Data Fusion, Google Cloud Storage, Dataproc, Composer, and Looker, Solid understanding of Google Cloud architecture, Strong work experience in SQL/Data warehousing, 4+ years of Working experience on tools like Spark, HBase, Sqoop, Impala, Kafka, Flume, Oozie, MapReduce, etc, GCP Architect certification will be an advantage, Google cloud Platform GCP Compute, Storage, Data, Network, Security,, Devops and Big Data, \nResponsibilities:\n Cloud ML Serverless Architecture and Security Cloud Functions, Cloud IAM, Design and build data engineering solutions using Google Cloud Platform (GCP) services: Tera data, BigQuery, DataFlow, Pub/Sub, BigTable, Data Fusion, and DataProc, Monitoring and improving the quality of the components throughout the development life cycle, Designing and Developing data modules that convert underlying raw data to more readily usable formats for reporting, Performing data deep dives to pinpoint Problems, Identify the application bottlenecks and opportunities to optimize performance and optimize the performance, Work on extracting, Loading, Transforming, cleaning, and validating data using cloud ETL/ELT tools, Work with the team to conduct training workshops to identify data sources, flows, and requirements, Preparing required project documentation and tracking and reporting regularly on the status of projects to all project stakeholders, Working with Business Analysts and other business teams to get the required data to build the semantic views, Periodically update senior management with the status of the project with excellent written and verbal communication skills, Troubleshoot production issues and coordinate with the support team for code deployment", "resume_similarity": null}